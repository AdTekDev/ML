
# ML.W06. NaÃ¯ve Bayesian Classification

- https://machinelearningcoban.com/2017/08/08/nbc/
- https://200lab.io/blog/tim-hieu-naive-bayes-classification-phan-1
- https://www.geeksforgeeks.org/machine-learning/naive-bayes-classifiers/
- https://www.ibm.com/think/topics/naive-bayes
- https://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html

## Code  
- https://scikit-learn.org/stable/modules/naive_bayes.html
- https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html
- https://www.datacamp.com/tutorial/naive-bayes-scikit-learn
- https://towardsdatascience.com/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6/
- https://jakevdp.github.io/PythonDataScienceHandbook/05.05-naive-bayes.html
- https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/

## Formula



[
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
]

---


[
\text{Posterior} = \frac{\text{Likelihood} \times \text{Prior}}{\text{Evidence}}
]

> ğŸ”¹ *Prior* = niá»m tin ban Ä‘áº§u - Prior Probability (XÃ¡c suáº¥t tiÃªn nghiá»‡m)
> ğŸ”¹ *Likelihood* = báº±ng chá»©ng á»§ng há»™ - Likelihood (Kháº£ nÄƒng xáº£y ra cá»§a báº±ng chá»©ng náº¿u giáº£ thuyáº¿t Ä‘Ãºng)
> ğŸ”¹ *Evidence* = Ä‘á»™ phá»• biáº¿n cá»§a báº±ng chá»©ng - Marginal Probability (XÃ¡c suáº¥t báº±ng chá»©ng) 
> ğŸ”¹ *Posterior* = niá»m tin sau khi cáº­p nháº­t - Posterior Probability (XÃ¡c suáº¥t háº­u nghiá»‡m) dá»±a trÃªn Hypothesis (Giáº£ thuyáº¿t) khi Ä‘Ã£ cÃ³ Dá»¯ liá»‡u hoáº·c quan sÃ¡t mÃ  ta cÃ³ Ä‘Æ°á»£c trÆ°á»›c Ä‘Ã³

---

